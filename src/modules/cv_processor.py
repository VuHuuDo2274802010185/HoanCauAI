# modules/cv_processor.py

import os  # x·ª≠ l√Ω t∆∞∆°ng t√°c v·ªõi h·ªá th·ªëng file v√† bi·∫øn m√¥i tr∆∞·ªùng
import re  # x·ª≠ l√Ω bi·ªÉu th·ª©c ch√≠nh quy
import json  # parse v√† dump JSON
import time  # x·ª≠ l√Ω th·ªùi gian v√† sleep retry
import logging  # ghi log
from typing import List, Dict, Optional  # khai b√°o ki·ªÉu

import pandas as pd  # x·ª≠ l√Ω DataFrame
pd.set_option("display.max_colwidth", None)  # hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß n·ªôi dung c√°c c·ªôt
import docx  # ƒë·ªçc file .docx

# --- Thi·∫øt l·∫≠p logger cho module ---
logger = logging.getLogger(__name__)  # l·∫•y logger theo t√™n module
logger.setLevel(logging.INFO)  # m·ª©c ƒë·ªô log t·ªëi thi·ªÉu INFO
fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")  # ƒë·ªãnh d·∫°ng log
# handler xu·∫•t log ra console
stream_h = logging.StreamHandler()  
stream_h.setFormatter(fmt)
logger.addHandler(stream_h)
# handler xu·∫•t log ra file trong th∆∞ m·ª•c log
from .config import LOG_DIR
LOG_DIR.mkdir(parents=True, exist_ok=True)
file_h = logging.FileHandler(LOG_DIR / "cv_processor.log", encoding="utf-8")
file_h.setFormatter(fmt)
logger.addHandler(file_h)

# --- C·∫•u h√¨nh extractor PDF: pdfminer, PyPDF2 ho·∫∑c PyMuPDF ---
_PDF_EX: Optional[str]
try:
    from pdfminer.high_level import extract_text
    _PDF_EX = "pdfminer"  # ∆∞u ti√™n pdfminer n·∫øu c√†i ƒë·∫∑t
except ImportError:
    try:
        import PyPDF2  # noqa: F401
        _PDF_EX = "pypdf2"  # fallback sang PyPDF2
    except ImportError:
        try:
            import fitz  # noqa: F401  # PyMuPDF
            _PDF_EX = "pymupdf"  # fallback sang PyMuPDF
        except ImportError:
            _PDF_EX = None  # kh√¥ng c√≥ th∆∞ vi·ªán PDF n√†o

from .llm_client import LLMClient  # client LLM m·∫∑c ƒë·ªãnh
from .config import ATTACHMENT_DIR, OUTPUT_CSV  # c·∫•u h√¨nh th∆∞ m·ª•c v√† file xu·∫•t
from .prompts import CV_EXTRACTION_PROMPT  # prompt LLM ƒë·ªÉ tr√≠ch xu·∫•t CV

class CVProcessor:
    """
    L·ªõp x·ª≠ l√Ω file CV: ƒë·ªçc text, g·ªçi LLM ho·∫∑c regex fallback, tr·∫£ v·ªÅ DataFrame
    """
    def __init__(self, fetcher: Optional[object] = None, llm_client: Optional[LLMClient] = None):
        """Kh·ªüi t·∫°o: c·∫•p fetcher (ƒë·ªçc email) v√† LLM client"""
        self.fetcher = fetcher  # ƒë·ªëi t∆∞·ª£ng c√≥ method fetch_cv_attachments()
        self.llm_client = llm_client or LLMClient()  # client LLM m·∫∑c ƒë·ªãnh

    def _extract_pdf(self, path: str) -> str:
        """
        ƒê·ªçc text t·ª´ file PDF b·∫±ng th∆∞ vi·ªán t∆∞∆°ng ·ª©ng
        Tr·∫£ v·ªÅ chu·ªói r·ªóng n·∫øu kh√¥ng c√≥ library
        """
        if _PDF_EX == "pdfminer":
            return extract_text(path)
        elif _PDF_EX == "pypdf2":
            from PyPDF2 import PdfReader
            txt = ""
            for p in PdfReader(path).pages:
                txt += p.extract_text() or ""
            return txt
        elif _PDF_EX == "pymupdf":
            import fitz
            doc = fitz.open(path)
            txt = "".join(page.get_text() for page in doc)
            doc.close()
            return txt
        logger.error("‚ùå Kh√¥ng c√≥ th∆∞ vi·ªán PDF ph√π h·ª£p ƒë·ªÉ tr√≠ch xu·∫•t text.")
        return ""

    def extract_text(self, path: str) -> str:
        """
        ƒê·ªçc vƒÉn b·∫£n t·ª´ file PDF ho·∫∑c DOCX
        Tr·∫£ v·ªÅ chu·ªói text, log c·∫£nh b√°o n·∫øu ƒë·ªãnh d·∫°ng kh√¥ng h·ªó tr·ª£
        """
        ext = os.path.splitext(path)[1].lower()  # l·∫•y ph·∫ßn m·ªü r·ªông
        try:
            if ext == ".pdf":
                return self._extract_pdf(path)
            if ext == ".docx":
                doc = docx.Document(path)
                return "\n".join(p.text for p in doc.paragraphs)
            logger.warning(f"‚ö†Ô∏è ƒê·ªãnh d·∫°ng kh√¥ng h·ªó tr·ª£: {path}")
        except Exception as e:
            logger.error(f"L·ªói khi ƒë·ªçc file {path}: {e}")
        return ""

    def extract_info_with_llm(self, text: str) -> Dict:
        """
        Enhanced LLM extraction with better error handling and retry logic
        """
        if not text.strip():
            logger.warning("Text input is empty for LLM extraction")
            return {}

        max_retries = 3
        base_delay = 1  # seconds
        
        for attempt in range(1, max_retries + 1):
            try:
                logger.info(f"LLM extraction attempt {attempt}/{max_retries}")
                
                # Create enhanced prompt with text length info
                text_length = len(text)
                if text_length > 8000:  # Truncate very long text
                    text = text[:8000] + "...[text truncated]"
                    logger.info(f"Text truncated from {text_length} to {len(text)} chars")
                
                # Generate response with timeout
                resp = self.llm_client.generate_content([CV_EXTRACTION_PROMPT, text])
                
                if not resp or not resp.strip():
                    raise ValueError(f"Empty response from LLM on attempt {attempt}")
                
                # Enhanced JSON extraction with multiple patterns
                json_data = self._extract_json_from_response(resp)
                
                if json_data:
                    logger.info(f"‚úÖ LLM extraction successful on attempt {attempt}")
                    return json_data
                else:
                    raise ValueError(f"No valid JSON found in LLM response on attempt {attempt}")
                    
            except Exception as e:
                logger.warning(f"‚ùå LLM attempt {attempt} failed: {e}")
                
                # Check for quota/rate limit errors
                error_msg = str(e).lower()
                if any(code in error_msg for code in ("quota", "429", "resource_exhausted", "rate limit")):
                    if attempt < max_retries:
                        delay = base_delay * (2 ** attempt)  # Exponential backoff
                        logger.warning(f"Quota/rate limit hit, retrying in {delay} seconds...")
                        time.sleep(delay)
                        continue
                
                if attempt == max_retries:
                    logger.error(f"All {max_retries} LLM attempts failed. Falling back to regex.")
                    return self._fallback_regex(text)
        
        # Fallback to regex if all attempts fail
        return self._fallback_regex(text)

    def _extract_json_from_response(self, response: str) -> Optional[Dict]:
        """Extract JSON from LLM response with multiple patterns"""
        try:
            # Pattern 1: JSON in code blocks
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            
            # Pattern 2: Direct JSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            
            # Pattern 3: Try to parse entire response as JSON
            return json.loads(response.strip())
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parsing failed: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error in JSON extraction: {e}")
            return None

    def _fallback_regex(self, text: str) -> Dict:
        """
        D√πng regex ƒë∆°n gi·∫£n ƒë·ªÉ tr√≠ch xu·∫•t c√°c tr∆∞·ªùng: t√™n, tu·ªïi, email, ƒëi·ªán tho·∫°i, h·ªçc v·∫•n, kinh nghi·ªám, ƒë·ªãa ch·ªâ, k·ªπ nƒÉng
        Tr·∫£ v·ªÅ dict v·ªõi c√°c key t∆∞∆°ng ·ª©ng
        """
        patterns = {
            "ten": r"(?:(?:H·ªç t√™n|T√™n)[:\-\s]+)([^\n]+)",
            "tuoi": r"(?:(?:Tu·ªïi|Age)[:\-\s]+)(\d{1,3})",
            "email": r"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)",
            "dien_thoai": r"(\+?\d[\d\-\s]{7,}\d)",
            "hoc_van": r"(?:(?:H·ªçc v·∫•n|Education)[:\-\s]+)([^\n]+)",
            "kinh_nghiem": r"(?:(?:Kinh nghi·ªám|Experience)[:\-\s]+)([^\n]+)",
            "dia_chi": r"(?:(?:ƒê·ªãa ch·ªâ|Address)[:\-\s]+)([^\n]+)",
            "ky_nang": r"(?:(?:K·ªπ nƒÉng|Skills?)[:\-\s]+)([^\n]+)",
        }
        info: Dict[str, str] = {}
        for k, p in patterns.items():
            m = re.search(p, text, re.IGNORECASE)
            info[k] = m.group(1).strip() if m else ""
        return info

    def process(self) -> pd.DataFrame:
        """
        T√¨m t·∫•t c·∫£ file CV (fetcher ho·∫∑c th∆∞ m·ª•c attachments), tr√≠ch xu·∫•t info, tr·∫£ v·ªÅ DataFrame
        """
        # fetch t·ª´ email n·∫øu c√≥ fetcher
        files: List[str] = self.fetcher.fetch_cv_attachments() if self.fetcher else []
        if not files:
            logger.info("üîç Kh√¥ng t√¨m th·∫•y qua fetcher, d√≤ th∆∞ m·ª•c attachments...")
            files = [
                os.path.join(ATTACHMENT_DIR, f)
                for f in os.listdir(ATTACHMENT_DIR)
                if f.lower().endswith((".pdf", ".docx"))
            ]
        if not files:
            logger.info("‚ÑπÔ∏è Kh√¥ng c√≥ file CV n√†o trong th∆∞ m·ª•c.")
            return pd.DataFrame()  # tr·∫£ v·ªÅ DataFrame r·ªóng n·∫øu kh√¥ng c√≥ file

        rows: List[Dict[str, str]] = []
        for path in files:
            txt = self.extract_text(path)  # ƒë·ªçc text file
            info = self.extract_info_with_llm(txt) or {}
            # gom th√¥ng tin v√†o dict
            rows.append({
                "Ngu·ªìn": os.path.basename(path),
                "H·ªç t√™n": info.get("ten", ""),
                "Tu·ªïi": info.get("tuoi", ""),
                "Email": info.get("email", ""),
                "ƒêi·ªán tho·∫°i": info.get("dien_thoai", ""),
                "ƒê·ªãa ch·ªâ": info.get("dia_chi", ""),
                "H·ªçc v·∫•n": info.get("hoc_van", ""),
                "Kinh nghi·ªám": info.get("kinh_nghiem", ""),
                "K·ªπ nƒÉng": info.get("ky_nang", ""),
            })

        df = pd.DataFrame(rows, columns=[
            "Ngu·ªìn",
            "H·ªç t√™n",
            "Tu·ªïi",
            "Email",
            "ƒêi·ªán tho·∫°i",
            "ƒê·ªãa ch·ªâ",
            "H·ªçc v·∫•n",
            "Kinh nghi·ªám",
            "K·ªπ nƒÉng",
        ])  # t·∫°o DataFrame t·ª´ list dict v·ªõi th·ª© t·ª± c·ªôt c·ªë ƒë·ªãnh
        return df  # tr·∫£ v·ªÅ k·∫øt qu·∫£

    def save_to_csv(self, df: pd.DataFrame, output: str = OUTPUT_CSV):
        """
        Ghi ƒë√® file CSV m·ªói l·∫ßn ch·∫°y; n·∫øu mu·ªën append, c√≥ th·ªÉ chuy·ªÉn mode v√† header
        """
        df.to_csv(output, index=False, encoding="utf-8-sig")  # l∆∞u file
        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(df)} h·ªì s∆° v√†o {output}")
