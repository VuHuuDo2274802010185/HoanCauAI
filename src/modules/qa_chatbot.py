"""H·ªó tr·ª£ h·ªèi ƒë√°p d·ªØ li·ªáu CV b·∫±ng API LLM."""

import json
import logging
import re
from datetime import datetime, timezone
import pandas as pd
from pathlib import Path
from typing import Optional, Dict, Any
from .dynamic_llm_client import DynamicLLMClient
from .config import CHAT_LOG_FILE, ATTACHMENT_DIR
import base64


class QAChatbot:
    """Simple wrapper around :func:`answer_question`."""

    def __init__(self, provider: str, model: str, api_key: str) -> None:
        self.provider = provider
        self.model = model
        self.api_key = api_key

    def ask_question(
        self,
        question: str,
        df: pd.DataFrame,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Delegate answering to :func:`answer_question`."""
        return answer_question(
            question,
            df,
            self.provider,
            self.model,
            self.api_key,
            context=context,
        )

# Enhanced logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def _make_file_link(fname: str) -> str:
    """Return an HTML download link for a CV file if it exists."""
    path = (ATTACHMENT_DIR / fname).resolve()
    if not path.exists():
        return fname
    data = base64.b64encode(path.read_bytes()).decode()
    mime = (
        "application/pdf"
        if path.suffix.lower() == ".pdf"
        else "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    )
    return f'<a download="{fname}" href="data:{mime};base64,{data}">{fname}</a>'


def _log_chat(question: str, answer: str, log_file: Path = CHAT_LOG_FILE) -> None:
    """Append a Q&A pair to the chat log file in JSON format with enhanced metadata."""
    entry = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "question": question,
        "answer": answer,
        "question_length": len(question),
        "answer_length": len(answer),
        "question_type": _classify_question(question)
    }
    try:
        # Ensure log directory exists
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        if log_file.exists():
            try:
                with open(log_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except json.JSONDecodeError:
                logger.warning("Chat log corrupted, recreating")
                data = []
        else:
            data = []
            
        data.append(entry)
        
        # Keep only last 1000 entries to prevent file from growing too large
        if len(data) > 1000:
            data = data[-800:]  # Keep last 800 entries
            
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"Chat logged: Q-type={entry['question_type']}, Q-len={entry['question_length']}, A-len={entry['answer_length']}")
    except Exception as e:
        logger.warning(f"Kh√¥ng th·ªÉ ghi log chat: {e}")


def _classify_question(question: str) -> str:
    """Classify question type for better analytics."""
    question_lower = question.lower()
    
    if any(word in question_lower for word in ["t√¨m", "ki·∫øm", "search", "find"]):
        return "search"
    elif any(word in question_lower for word in ["th·ªëng k√™", "stats", "count", "s·ªë l∆∞·ª£ng"]):
        return "statistics"
    elif any(word in question_lower for word in ["so s√°nh", "compare", "kh√°c nhau"]):
        return "comparison"
    elif any(word in question_lower for word in ["t√≥m t·∫Øt", "summary", "overview"]):
        return "summary"
    elif any(word in question_lower for word in ["k·ªπ nƒÉng", "skill", "nƒÉng l·ª±c"]):
        return "skills"
    elif any(word in question_lower for word in ["kinh nghi·ªám", "experience", "c√¥ng vi·ªác"]):
        return "experience"
    elif any(word in question_lower for word in ["h·ªçc v·∫•n", "education", "b·∫±ng c·∫•p"]):
        return "education"
    else:
        return "general"


def _is_time_question(question: str) -> bool:
    """Detect if the user is asking about current time or date."""
    question_lower = question.lower()
    patterns = [
        r"b√¢y gi·ªù", r"m·∫•y gi·ªù", r"th·ªùi gian hi·ªán t·∫°i", r"current time",
        r"current date", r"ng√†y bao nhi√™u", r"h√¥m nay" 
    ]
    return any(re.search(p, question_lower) for p in patterns)


def _create_enhanced_prompt(question: str, df: pd.DataFrame, context: Optional[Dict[str, Any]] = None) -> list:
    """Create enhanced prompt with better context and instructions."""
    
    # Get data summary
    total_records = len(df)
    columns = list(df.columns)
    
    # Sample data for context
    
    # Create enhanced system prompt
    system_prompt = f"""B·∫°n l√† Tr·ª£ l√Ω AI chuy√™n nghi·ªáp ph√¢n t√≠ch d·ªØ li·ªáu CV v√† tuy·ªÉn d·ª•ng.

TH√îNG TIN D·ªÆ LI·ªÜU:
- T·ªïng s·ªë h·ªì s∆°: {total_records}
- C√°c tr∆∞·ªùng d·ªØ li·ªáu: {', '.join(columns)}

NHI·ªÜM V·ª§:
- Ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu CV
- Cung c·∫•p th√¥ng tin ch√≠nh x√°c, chi ti·∫øt v√† h·ªØu √≠ch
- S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng markdown khi c·∫ßn thi·∫øt
- ƒê∆∞a ra s·ªë li·ªáu c·ª• th·ªÉ khi c√≥ th·ªÉ

QUY T·∫ÆC:
- Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ
- Cung c·∫•p g·ª£i √Ω khi ph√π h·ª£p
- S·ª≠ d·ª•ng emoji ƒë·ªÉ l√†m cho c√¢u tr·∫£ l·ªùi sinh ƒë·ªông h∆°n"""
    
    # Data context - use entire dataset instead of a preview
    data_context = f"To√†n b·ªô d·ªØ li·ªáu ({total_records} h·ªì s∆°):\n\n{df.to_csv(index=False)}"
    
    # Question with context
    question_with_context = f"C√¢u h·ªèi: {question}"
    
    if context:
        question_with_context += f"\n\nTh√¥ng tin b·ªï sung: {context}"
    
    return [system_prompt, data_context, question_with_context]


def answer_question(question: str, df: pd.DataFrame, provider: str, model: str, api_key: str, context: Optional[Dict[str, Any]] = None) -> str:
    """Enhanced question answering with better error handling and context."""
    
    # Input validation
    if not question or not question.strip():
        raise ValueError("C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
    
    if df is None or df.empty:
        raise ValueError("Dataset CV tr·ªëng. Vui l√≤ng x·ª≠ l√Ω CV tr∆∞·ªõc khi s·ª≠ d·ª•ng t√≠nh nƒÉng chat.")
    
    if not api_key or not api_key.strip():
        raise ValueError("API key kh√¥ng h·ª£p l·ªá")
    
    question = question.strip()
    logger.info(f"Processing question: {question[:100]}... (Total records: {len(df)})")

    # Return system time immediately if asked
    if _is_time_question(question):
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        answer = f"B√¢y gi·ªù l√† {now}"
        _log_chat(question, answer)
        return answer
    
    try:
        # Create enhanced prompt
        messages = _create_enhanced_prompt(question, df, context)
        
        # Generate response
        client = DynamicLLMClient(provider=provider, model=model, api_key=api_key)
        answer = client.generate_content(messages)
        
        if not answer or not answer.strip():
            raise ValueError("AI kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
        
        answer = answer.strip()
        
        # Post-process answer and attach CV links to candidate names
        answer = _post_process_answer(answer)
        answer = _link_names_to_cv(answer, df)
        
        # Log the interaction
        _log_chat(question, answer)
        
        logger.info(f"Question answered successfully. Answer length: {len(answer)}")
        return answer
        
    except Exception as e:
        logger.error(f"Error answering question: {e}")
        error_msg = f"Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. L·ªói: {str(e)}"
        _log_chat(question, error_msg)
        return error_msg


def _post_process_answer(answer: str) -> str:
    """Post-process AI answer to improve formatting and readability."""
    
    # Clean up extra whitespace
    answer = re.sub(r'\n\s*\n\s*\n', '\n\n', answer)
    answer = answer.strip()
    
    # Add proper spacing around headers
    answer = re.sub(r'^(#{1,6})\s*(.+)', r'\1 \2', answer, flags=re.MULTILINE)
    
    # Ensure proper bullet point formatting
    answer = re.sub(r'^\s*[-*+]\s*', '‚Ä¢ ', answer, flags=re.MULTILINE)

    # Add emoji if not present and answer is positive
    if not re.search(r'[üòÄ-üôè]', answer) and len(answer) > 50:
        if any(word in answer.lower() for word in ['t√¨m th·∫•y', 'c√≥', 'th√†nh c√¥ng', 'ƒë∆∞·ª£c']):
            answer = "‚úÖ " + answer
        elif any(word in answer.lower() for word in ['kh√¥ng', 'ch∆∞a', 'thi·∫øu']):
            answer = "‚ÑπÔ∏è " + answer

    # Replace file names with download links
    file_pattern = re.compile(r'([\w\s.-]+\.(?:pdf|docx?))', re.IGNORECASE)

    def repl(match: re.Match) -> str:
        fname = match.group(1).strip()
        return _make_file_link(fname)

    answer = file_pattern.sub(repl, answer)

    return answer


def _link_names_to_cv(answer: str, df: pd.DataFrame) -> str:
    """Attach CV file links next to candidate names mentioned in the answer."""
    if not {'H·ªç t√™n', 'Ngu·ªìn'} <= set(df.columns):
        return answer

    name_to_file = (
        df[['H·ªç t√™n', 'Ngu·ªìn']]
        .dropna()
        .astype(str)
        .apply(lambda x: (x['H·ªç t√™n'].strip(), x['Ngu·ªìn'].strip()), axis=1)
        .tolist()
    )

    for name, fname in name_to_file:
        if not name or not fname or name not in answer:
            continue
        link = _make_file_link(fname)
        pattern = re.compile(rf"\b{re.escape(name)}\b")
        answer = pattern.sub(f"{name} ({link})", answer, count=1)

    return answer


def get_chat_statistics(log_file: Path = CHAT_LOG_FILE) -> Dict[str, Any]:
    """Get chat statistics from log file."""
    try:
        if not log_file.exists():
            return {"total_chats": 0, "question_types": {}, "average_lengths": {}}
        
        try:
            with open(log_file, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError:
            logger.warning("Chat log corrupted, ignoring statistics")
            return {"total_chats": 0, "question_types": {}, "average_lengths": {}}
        
        if not data:
            return {"total_chats": 0, "question_types": {}, "average_lengths": {}}
        
        # Calculate statistics
        total_chats = len(data)
        question_types = {}
        total_question_length = 0
        total_answer_length = 0
        
        for entry in data:
            q_type = entry.get("question_type", "general")
            question_types[q_type] = question_types.get(q_type, 0) + 1
            total_question_length += entry.get("question_length", 0)
            total_answer_length += entry.get("answer_length", 0)
        
        avg_question_length = total_question_length / total_chats if total_chats > 0 else 0
        avg_answer_length = total_answer_length / total_chats if total_chats > 0 else 0
        
        return {
            "total_chats": total_chats,
            "question_types": question_types,
            "average_lengths": {
                "question": round(avg_question_length, 1),
                "answer": round(avg_answer_length, 1)
            },
            "last_chat": data[-1]["timestamp"] if data else None
        }
        
    except Exception as e:
        logger.error(f"Error getting chat statistics: {e}")
        return {"total_chats": 0, "question_types": {}, "average_lengths": {}, "error": str(e)}

