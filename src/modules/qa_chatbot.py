"""Há»— trá»£ há»i Ä‘Ã¡p dá»¯ liá»‡u CV báº±ng API LLM."""

import json
import logging
import re
from datetime import datetime, timezone
import pandas as pd
from pathlib import Path
from typing import Optional, Dict, Any
from .dynamic_llm_client import DynamicLLMClient
from .config import CHAT_LOG_FILE, ATTACHMENT_DIR
import base64


class QAChatbot:
    """Simple wrapper around :func:`answer_question`."""

    def __init__(self, provider: str, model: str, api_key: str) -> None:
        self.provider = provider
        self.model = model
        self.api_key = api_key

    def ask_question(
        self,
        question: str,
        df: pd.DataFrame,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Delegate answering to :func:`answer_question`."""
        return answer_question(
            question,
            df,
            self.provider,
            self.model,
            self.api_key,
            context=context,
        )

# Enhanced logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def _make_file_link(fname: str) -> str:
    """Return an HTML download link for a CV file if it exists."""
    path = (ATTACHMENT_DIR / fname).resolve()
    if not path.exists():
        return fname
    data = base64.b64encode(path.read_bytes()).decode()
    mime = (
        "application/pdf"
        if path.suffix.lower() == ".pdf"
        else "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    )
    return f'<a download="{fname}" href="data:{mime};base64,{data}">{fname}</a>'


def _log_chat(question: str, answer: str, log_file: Path = CHAT_LOG_FILE) -> None:
    """Append a Q&A pair to the chat log file in JSON format with enhanced metadata."""
    entry = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "question": question,
        "answer": answer,
        "question_length": len(question),
        "answer_length": len(answer),
        "question_type": _classify_question(question)
    }
    try:
        # Ensure log directory exists
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        if log_file.exists():
            try:
                with open(log_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except json.JSONDecodeError:
                logger.warning("Chat log corrupted, recreating")
                data = []
        else:
            data = []
            
        data.append(entry)
        
        # Keep only last 1000 entries to prevent file from growing too large
        if len(data) > 1000:
            data = data[-800:]  # Keep last 800 entries
            
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"Chat logged: Q-type={entry['question_type']}, Q-len={entry['question_length']}, A-len={entry['answer_length']}")
    except Exception as e:
        logger.warning(f"KhÃ´ng thá»ƒ ghi log chat: {e}")


def _classify_question(question: str) -> str:
    """Classify question type for better analytics."""
    question_lower = question.lower()
    
    if any(word in question_lower for word in ["tÃ¬m", "kiáº¿m", "search", "find"]):
        return "search"
    elif any(word in question_lower for word in ["thá»‘ng kÃª", "stats", "count", "sá»‘ lÆ°á»£ng"]):
        return "statistics"
    elif any(word in question_lower for word in ["so sÃ¡nh", "compare", "khÃ¡c nhau"]):
        return "comparison"
    elif any(word in question_lower for word in ["tÃ³m táº¯t", "summary", "overview"]):
        return "summary"
    elif any(word in question_lower for word in ["ká»¹ nÄƒng", "skill", "nÄƒng lá»±c"]):
        return "skills"
    elif any(word in question_lower for word in ["kinh nghiá»‡m", "experience", "cÃ´ng viá»‡c"]):
        return "experience"
    elif any(word in question_lower for word in ["há»c váº¥n", "education", "báº±ng cáº¥p"]):
        return "education"
    else:
        return "general"


def _is_time_question(question: str) -> bool:
    """Detect if the user is asking about current time or date."""
    question_lower = question.lower()
    patterns = [
        r"bÃ¢y giá»", r"máº¥y giá»", r"thá»i gian hiá»‡n táº¡i", r"current time",
        r"current date", r"ngÃ y bao nhiÃªu", r"hÃ´m nay" 
    ]
    return any(re.search(p, question_lower) for p in patterns)


def _create_enhanced_prompt(question: str, df: pd.DataFrame, context: Optional[Dict[str, Any]] = None) -> list:
    """Create enhanced prompt with better context and instructions."""
    
    # Get data summary
    total_records = len(df)
    columns = list(df.columns)
    
    # Sample data for context
    
    # Create enhanced system prompt
    system_prompt = f"""Báº¡n lÃ  Trá»£ lÃ½ AI chuyÃªn nghiá»‡p phÃ¢n tÃ­ch dá»¯ liá»‡u CV vÃ  tuyá»ƒn dá»¥ng.

THÃ”NG TIN Dá»® LIá»†U:
- Tá»•ng sá»‘ há»“ sÆ¡: {total_records}
- CÃ¡c trÆ°á»ng dá»¯ liá»‡u: {', '.join(columns)}

NHIá»†M Vá»¤:
- PhÃ¢n tÃ­ch vÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn dá»¯ liá»‡u CV
- Cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c, chi tiáº¿t vÃ  há»¯u Ã­ch
- Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng markdown khi cáº§n thiáº¿t
- ÄÆ°a ra sá»‘ liá»‡u cá»¥ thá»ƒ khi cÃ³ thá»ƒ

QUY Táº®C:
- LuÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
- Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y nÃ³i rÃµ
- Cung cáº¥p gá»£i Ã½ khi phÃ¹ há»£p
- Sá»­ dá»¥ng emoji Ä‘á»ƒ lÃ m cho cÃ¢u tráº£ lá»i sinh Ä‘á»™ng hÆ¡n"""
    
    # Data context - use entire dataset instead of a preview
    data_context = f"ToÃ n bá»™ dá»¯ liá»‡u ({total_records} há»“ sÆ¡):\n\n{df.to_csv(index=False)}"
    
    # Question with context
    question_with_context = f"CÃ¢u há»i: {question}"
    
    if context:
        question_with_context += f"\n\nThÃ´ng tin bá»• sung: {context}"
    
    return [system_prompt, data_context, question_with_context]


def answer_question(question: str, df: pd.DataFrame, provider: str, model: str, api_key: str, context: Optional[Dict[str, Any]] = None) -> str:
    """Enhanced question answering with better error handling and context."""
    
    # Input validation
    if not question or not question.strip():
        raise ValueError("CÃ¢u há»i khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng")
    
    if df is None or df.empty:
        raise ValueError("Dataset CV trá»‘ng. Vui lÃ²ng xá»­ lÃ½ CV trÆ°á»›c khi sá»­ dá»¥ng tÃ­nh nÄƒng chat.")
    
    if not api_key or not api_key.strip():
        raise ValueError("API key khÃ´ng há»£p lá»‡")
    
    question = question.strip()
    logger.info(f"Processing question: {question[:100]}... (Total records: {len(df)})")

    # Return system time immediately if asked
    if _is_time_question(question):
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        answer = f"BÃ¢y giá» lÃ  {now}"
        _log_chat(question, answer)
        return answer
    
    try:
        # Create enhanced prompt
        messages = _create_enhanced_prompt(question, df, context)
        
        # Generate response
        client = DynamicLLMClient(provider=provider, model=model, api_key=api_key)
        answer = client.generate_content(messages)
        
        if not answer or not answer.strip():
            raise ValueError("AI khÃ´ng tráº£ vá» káº¿t quáº£")
        
        answer = answer.strip()
        
        # Post-process answer
        answer = _post_process_answer(answer)
        
        # Log the interaction
        _log_chat(question, answer)
        
        logger.info(f"Question answered successfully. Answer length: {len(answer)}")
        return answer
        
    except Exception as e:
        logger.error(f"Error answering question: {e}")
        error_msg = f"Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y. Lá»—i: {str(e)}"
        _log_chat(question, error_msg)
        return error_msg


def _post_process_answer(answer: str) -> str:
    """Post-process AI answer to improve formatting and readability."""
    
    # Clean up extra whitespace
    answer = re.sub(r'\n\s*\n\s*\n', '\n\n', answer)
    answer = answer.strip()
    
    # Add proper spacing around headers
    answer = re.sub(r'^(#{1,6})\s*(.+)', r'\1 \2', answer, flags=re.MULTILINE)
    
    # Ensure proper bullet point formatting
    answer = re.sub(r'^\s*[-*+]\s*', 'â€¢ ', answer, flags=re.MULTILINE)

    # Add emoji if not present and answer is positive
    if not re.search(r'[ðŸ˜€-ðŸ™]', answer) and len(answer) > 50:
        if any(word in answer.lower() for word in ['tÃ¬m tháº¥y', 'cÃ³', 'thÃ nh cÃ´ng', 'Ä‘Æ°á»£c']):
            answer = "âœ… " + answer
        elif any(word in answer.lower() for word in ['khÃ´ng', 'chÆ°a', 'thiáº¿u']):
            answer = "â„¹ï¸ " + answer

    # Replace file names with download links
    file_pattern = re.compile(r'([\w\s.-]+\.(?:pdf|docx?))', re.IGNORECASE)

    def repl(match: re.Match) -> str:
        fname = match.group(1).strip()
        return _make_file_link(fname)

    answer = file_pattern.sub(repl, answer)

    return answer


def get_chat_statistics(log_file: Path = CHAT_LOG_FILE) -> Dict[str, Any]:
    """Get chat statistics from log file."""
    try:
        if not log_file.exists():
            return {"total_chats": 0, "question_types": {}, "average_lengths": {}}
        
        try:
            with open(log_file, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError:
            logger.warning("Chat log corrupted, ignoring statistics")
            return {"total_chats": 0, "question_types": {}, "average_lengths": {}}
        
        if not data:
            return {"total_chats": 0, "question_types": {}, "average_lengths": {}}
        
        # Calculate statistics
        total_chats = len(data)
        question_types = {}
        total_question_length = 0
        total_answer_length = 0
        
        for entry in data:
            q_type = entry.get("question_type", "general")
            question_types[q_type] = question_types.get(q_type, 0) + 1
            total_question_length += entry.get("question_length", 0)
            total_answer_length += entry.get("answer_length", 0)
        
        avg_question_length = total_question_length / total_chats if total_chats > 0 else 0
        avg_answer_length = total_answer_length / total_chats if total_chats > 0 else 0
        
        return {
            "total_chats": total_chats,
            "question_types": question_types,
            "average_lengths": {
                "question": round(avg_question_length, 1),
                "answer": round(avg_answer_length, 1)
            },
            "last_chat": data[-1]["timestamp"] if data else None
        }
        
    except Exception as e:
        logger.error(f"Error getting chat statistics: {e}")
        return {"total_chats": 0, "question_types": {}, "average_lengths": {}, "error": str(e)}

